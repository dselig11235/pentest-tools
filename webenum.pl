#!/usr/bin/perl

use Getopt::Std;
use Digest::SHA qw(sha512_base64);

use File::Spec;
use File::Basename;
my $FILE = __FILE__;
$SCRIPT = `readlink -f $FILE`;
$DIR = dirname($SCRIPT);

sub path2url {
    my $rv = shift;
    $rv =~ s/([^a-z\d\Q.-_~\/\E])/sprintf("%%%2.2X", ord($1))/geix;
    $rv =~ tr/ /+/;
    return $rv;
}

sub url_encode {
    my $rv = shift;
    $rv =~ s/([^a-z\d\Q.-_~ \E])/sprintf("%%%2.2X", ord($1))/geix;
    $rv =~ tr/ /+/;
    return $rv;
}


sub url_decode {
    my $rv = shift;
    $rv =~ tr/+/ /;
    $rv =~ s/\%([a-f\d]{2})/ pack 'C', hex $1 /geix;
    return $rv;
}

sub encode {
    my $target = shift;
    $target =~ tr'/'%';
    return $target;
}

sub decode {
    my $file = shift;
    $file =~ tr'%'/';
    return $target;
}

sub printToFile {
    my ($filename, $msg) = @_;
    open(my $out, ">", "$filename")
        or die "Couldn't open file '$filename'";
    print $out $msg;
    close $out;
}

if(! -e 'targets.web') {
    if(! -e 'targets.http' and ! -e 'targets.https') {
        die "Cant't find targets.web, targets.http, or targets.https";
    }
    open(WebFile, '>', 'targets.web')
        or die "Can't open targets.web for writing";
    if(open(HTTPFile, '<', 'targets.http')) {
        while(<HTTPFile>) {
            if($_ =~ m'(.*):(\d+)/tcp') {
                print WebFile "http://$1:$2\n";
            }
        }
    }
    close HTTPFile;
    if(open(HTTPSFile, '<', 'targets.https')) {
        while(<HTTPSFile>) {
            if($_ =~ m'(.*):(\d+)/tcp') {
                print WebFile "https://$1:$2\n";
            }
        }
    }
    close HTTPSFile;
    close WebFile;
}

open(my $tfile, '<', 'targets.web')
    or die "Couldn't open file targets.web";

$TIMEOUT = 15;
while(<$tfile>) {
    chomp;
    print "Processing $_\n";
    my $encoded = encode($_);
    my $dir = "web.$encoded";
    system("mkdir", "-p", $dir);
    my $info = `curl -kLsm $TIMEOUT -o "$dir/index.html" -D "$dir/headers" -w "%{url_effective},%{http_code}" "$_"`;
    print "\tINFO: $info\n";
    system("curl", "-m", "$TIMEOUT", "-kfLso", "$dir/robots.txt", "$_/robots.txt.png");
    if($? == 0) {
        print "\tfound robots.txt\n";
        system("$DIR/webshot.py", "$_/robots.txt", "$dir/robots.txt on $encoded%robots.txt.png");
    } else {
        print "\tno robots.txt\n";
    }
    #print qq(curl -kLs -o "$dir/index.html" -D "$dir/headers" -w "%{url_effective},%{http_code}" "$_"), "\n";
    @info = split(/,\s*/, $info);
    $abspath = File::Spec->rel2abs($dir);
    $file_url = path2url("$abspath/index.html");
    #
    #Take screenshot of index page
    #Screenshots were taking forever if the page is unavailable, so I tried
    #just taking screenshots of the file I downloaded with curl, but relative
    #links don't work in webpages with this method
    #system("$DIR/webshot.py", "file://$file_url", "$dir/Manual Navigation to $encoded.png");

    #So instead, we'll use the status code to figure out if the page was
    #accessible.  If it was, we'll screenshot it
    if($info[1] != 0) {
        system("$DIR/webshot.py", $_, "$dir/Manual Navigation to $encoded.png");
    }
    print "\tTook screenshot of index page\n";
    printToFile("$dir/url_effective", $info[0]);
    printToFile("$dir/code", $info[1]);
    if(open(my $htmlfile, "<", "$dir/index.html")) {
        my $html = do {
            local $/ = undef;
            <$htmlfile>;
        };
        printToFile("$dir/sha", sha512_base64($html));
        if($html =~ m'<title>(.*)</title>'sm) {
            print "\tTITLE: $1\n";
            printToFile("$dir/title", $1);
        } else {
            print "\ttitle not found\n";
            printToFile("$dir/title", '');
        }
        printToFile("$dir/inputs", join("\n", $html =~ /(<input.*?>)/sg))
    }
    close $index;
    close $out;
}
